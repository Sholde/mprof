#+TITLE: MPI Profiler
#+AUTHOR: Sholde
#+DATE: April 2021

*MPI Profiler* (alias mprof) is a tool to profile *MPI* function. It use an
*interposition library*, nammed *libmprof.so*. A wrapper is also provided, nammed
*mprof*.

* Disclaimer

  This tool profile *only MPI_COMM_WORLD communication* (other communication
  will work but not profiled) and it is not currently *thread safe*.

  Therefore, we recommend you to run this tool only with full *MPI* application
  and with only *MPI_COMM_WORLD* communicator.

* Dependencies

  Please ensure that following dependencies are installed in your system:

  - gcc
  - make
  - MPI library (any vesrion)
  - coreutils (install)
  - pandoc (for manpage)

** Example of installation on Arch Linux

   #+BEGIN_SRC shell
$ sudo pacman -S --needed gcc make openmpi coreutils pandoc
   #+END_SRC

* Installation

  #+BEGIN_SRC shell
$ git clone https://github.com/Sholde/mprof.git
$ cd mprof
$ make
$ sudo make install
  #+END_SRC

* Usage

  It is easy to use. You just need to put *mprof* word ahead your *MPI*
  execution line. For example:

  #+BEGIN_SRC shell
$ mprof mpiexec -np 4 ./a.out
or
$ mprof mpirun -np 4 ./a.out
  #+END_SRC

* Environment variable

   *mprof* allow you to specify in a environment variable options, nammed
   *MPROF_OPTIONS*.

** List of options

   - ~--verbose~ print all *MPI* functions interposed with the process rank of
     the caller
   - ~--barrier~ print only *MPI_Barrier* call
   - ~--finalize~ print only *MPI_Finalize* call
   - ~--profile~ print only profiling information, it is a verbose mode for
     profiling information

** Example of usage with deadlock

   Here it is a code where only the *process 0* call *MPI_Barrier* and others
   finish.

   #+BEGIN_SRC shell
$ MPROF_OPTIONS="--finalize" mprof mpirun -np 4 ./lbm
=================== CONFIG ===================
iterations           = 16
width                = 800
height               = 160
obstacle_r           = 17.000000
obstacle_x           = 161.000000
obstacle_y           = 83.000000
reynolds             = 100.000000
reynolds             = 100.000000
inflow_max_velocity  = 0.100000
output_filename      = result.raw
write_interval       = 50
------------ Derived parameters --------------
kinetic_viscosity    = 0.034000
relax_parameter      = 1.661130
==============================================
 RANK 0 ( LEFT -1 RIGHT -1 TOP -1 BOTTOM 1 CORNER -1, -1, -1, -1 ) ( POSITION 0 0 ) (WH 802 42 ) 
 RANK 2 ( LEFT -1 RIGHT -1 TOP 1 BOTTOM 3 CORNER -1, -1, -1, -1 ) ( POSITION 0 80 ) (WH 802 42 ) 
 RANK 1 ( LEFT -1 RIGHT -1 TOP 0 BOTTOM 2 CORNER -1, -1, -1, -1 ) ( POSITION 0 40 ) (WH 802 42 ) 
 RANK 3 ( LEFT -1 RIGHT -1 TOP 2 BOTTOM -1 CORNER -1, -1, -1, -1 ) ( POSITION 0 120 ) (WH 802 42 ) 
Progress [    1 /    16]
Progress [    2 /    16]
Progress [    3 /    16]
Progress [    4 /    16]
Progress [    5 /    16]
Progress [    6 /    16]
Progress [    7 /    16]
Progress [    8 /    16]
Progress [    9 /    16]
Progress [   10 /    16]
Progress [   11 /    16]
Progress [   12 /    16]
Progress [   13 /    16]
Progress [   14 /    16]
Progress [   15 /    16]
==mprof== Process 3 enter in MPI_Finalize
==mprof== Process 1 enter in MPI_Finalize
==mprof== Process 2 enter in MPI_Finalize
^C$
   #+END_SRC

** Example of usage with contigous send

   A little benchmark where *Process 0* send to *Process 1* an array of *2
   integer* but *one by one*.

   #+BEGIN_SRC shell
$ MPROF_OPTIONS="--profile" mprof mpirun -np 2 ./a.out
==mprof== PROFILE: Process 0 send independently elements which are contiguous to Process 1
==mprof==             - Sending 1 element(s) of MPI_INT
===============================================================================
================================= MPI PROFILER ================================
===============================================================================
==mprof== GLOBAL SUMMARY:
==mprof==               running: 74 us
==mprof==          message sent: 2 msg take 8 bytes - waiting 26 us in total
==mprof==          message recv: 2 msg take 8 bytes - waiting 44 us in total
==mprof==     barrier(s) passed: 0 - waiting 0 ns in total
==mprof==            warning(s): 1 - 1 contiguous send
==mprof== 
==mprof== LOCAL SUMMARY (Process 0):
==mprof==               running: 46 us
==mprof==          message sent: 2 msg take 8 bytes - waiting 26 us (max: 25 us)
==mprof==          message recv: 0 msg take 0 bytes - waiting 0 ns (max: 0 ns)
==mprof==     barrier(s) passed: 0 - waiting 0 ns (max: 0 ns)
==mprof==       list(s) sent to: 1
==mprof==     list(s) recv from:
==mprof==            warning(s): 1 - 1 contiguous send
==mprof== 
==mprof== LOCAL SUMMARY (Process 1):
==mprof==               running: 74 us
==mprof==          message sent: 0 msg take 0 bytes - waiting 0 ns (max: 0 ns)
==mprof==          message recv: 2 msg take 8 bytes - waiting 44 us (max: 35 us)
==mprof==     barrier(s) passed: 0 - waiting 0 ns (max: 0 ns)
==mprof==       list(s) sent to:
==mprof==     list(s) recv from: 0
==mprof== 
==mprof== ERROR SUMMARY:
==mprof==          No errors
   #+END_SRC shell
